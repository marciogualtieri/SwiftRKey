---
title: "SwitftRKey: A typing prediction app written in R"
author: "Marcio Gualtieri (marcio dot gualtieri at gmail dot com)"
output:
  html_document:
    css: ./styles/style.css
    df_print: kable
    toc: yes
    toc_depth: 6
  md_document:
    df_print: kable
    toc: yes
    toc_depth: 6
    variant: markdown_github
---

## Installing Dependencies

You will need to install some libraries in your operating system before you install the required packages.

### WordNet

Follow the instructions for Linux:

    sudo apt-get install wordnet

### LibV8

Follow the instructions for Linux:

    sudo apt-get install libv8-dev

This library is required by the [pluralize" package](https://github.com/hrbrmstr/pluralize).

## Installing the Required Packages

You might need to install the following packages if you don't already have them:

```{r, eval = FALSE}
install.packages("dplyr")
install.packages("R.utils")
install.packages("tm")
install.packages("wordcloud")
install.packages("SnowballC")
install.packages("ggplot2")
install.packages("codetools")
install.packages("RWeka")
install.packages("textcat")
install.packages("ff")
install.packages("bigmemory", repos = "http://R-Forge.R-project.org")
install.packages("qdap")
install.packages("textcat")
install.packages("data.table")
install.packages("wordnet")
install.packages("pacman")
```

Just run the commands for the packages you need or run the whole chunk before you run the remaining ones in this notebook.

## Importing the Required Packages

Once the libraries are installed, they need to be loaded as follows:

```{r}
suppressMessages(library(dplyr))
suppressMessages(library(R.utils))
suppressMessages(library(tm))
suppressMessages(library(wordcloud))
suppressMessages(library(SnowballC))
suppressMessages(library(ggplot2))
suppressMessages(library(RWeka))
suppressMessages(library(textcat))
suppressMessages(library(ff))
suppressMessages(library(bigmemory))
suppressMessages(library(qdap))
suppressMessages(library(textcat))
suppressMessages(library(data.table))
suppressMessages(library(wordnet))
suppressMessages(library(pacman))
p_load_gh('hrbrmstr/pluralize')
p_load(quanteda)
```

Wordnet also requires the following setup:

```{r}
Sys.setenv(WNHOME = "/usr/bin/wordnet")
wordnet_path <- file.path("usr", "share", "dict")
setDict(wordnet_path)
```

## Knitr Settings

The raw data-set is pretty big, which might make rendering the output slow, so let's enable knitr's global caching:

```{r}
knitr::opts_chunk$set(cache = TRUE)
```

I'm also enabling verbose for easier debugging:

```{r}
knitr::opts_knit$set(verbose = TRUE)
```

## Setting a Seed

To make sure this notebook is reproducible:

```{r}
set.seed(123)
```

## Synopsis

The objective of this notebook is basically data cleaning and exploratory data analysis. Before we can build a predictive model for keyboard typing, i.e., what is the next word given the current typed words, we are going to use some of R's text-mining tools to find basic patterns in English text data, i.e., the distribution of words and relationships between them in sentences present in the raw data-set.

Our work-horse is going to be the [tm](http://tm.r-forge.r-project.org/) package, but we are also going to need other auxiliary packages to get the job done (such as [RWeka](https://www.rdocumentation.org/packages/RWeka/versions/0.4-34), [wordcloud](https://www.rdocumentation.org/packages/wordcloud/versions/2.5), [textcat](https://www.rdocumentation.org/packages/textcat/versions/1.0-4) and [qdap](https://github.com/trinker/qdap) to cite a few).

## Data Processing

### Downloading the Data-set

```{r}
download_zipped_data <- function(url, destination) {
  temp_file <- tempfile()
  download.file(url, temp_file)
  unzip(zipfile = temp_file, exdir = destination)
  unlink(temp_file)
}

download_zipped_data("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip",
                     "./data")
```

### Data-set's Files

```{r}
list.files(path = "./data", recursive = TRUE)
```

There seem to be three different categories of text ("blogs", "news" and "twitter") for four different languages(German, English, finnish and Russian).

Here's a sample from "blogs":

```{r}
file_head <- function(name, n = 3) {
    connection <- file(name, open = "r")
    result <- readLines(connection, n = n)
    close(connection)
    result
}

file_head("./data/final/en_US/en_US.blogs.txt")
```

A sample from "news":

```{r}
file_head("./data/final/en_US/en_US.news.txt")
```

A sample from "twitter":

```{r}
file_head("./data/final/en_US/en_US.twitter.txt")
```

Helps to get some basic metadata on the files we are working with:

```{r}
file_size <- function(name)
    suppressWarnings(utils:::format.object_size(file.info(name)[["size"]], "auto"))

file_lines <- function(name)
    suppressWarnings(countLines(name))

longest_line <- function(name)
    suppressWarnings(max(sapply(readLines(name), nchar)))

files_info <- function(dir) {
    files <- list.files(dir, recursive = TRUE, full.names = TRUE)
    data.frame(size = sapply(files, file_size),
               lines = sapply(files, file_lines),
               longest_line = sapply(files, longest_line))
}

files_info("./data/final/en_US")
```

### Sampling from the Input Files

As we have found out in the previous section, the files are huge, thus we are going to work with a sub-set from the available data. The following code is going to take random samples from the available text files:

```{r}
sample_file <- function(input_file, fraction) {
    connection <- file(input_file, open = "r")
    lines <- readLines(connection, skipNul = TRUE)
    number_lines <- length(lines)
    close(connection)
    lines[rbinom(as.integer(number_lines * fraction), number_lines, 0.5)]
}

create_sample_file <- function(input_file, output_dir, fraction) {
    sample <- sample_file(input_file, fraction)
    dir.create(output_dir, recursive = TRUE, showWarnings = FALSE)
    output_file <- paste0(output_dir, "/", basename(input_file))
    writeLines(sample, output_file)
}

create_sample_files <- function(input_dir, output_dir, fraction) {
     files <- list.files(input_dir, recursive = TRUE, full.names = TRUE)
     invisible(sapply(files, create_sample_file, output_dir = output_dir, fraction = fraction))
}

create_sample_files("./data/final/en_US", "./data/samples", 0.0001)
```

We are going to take 0.01% of the sentences inside each file in the directory. I have verified by trial and error that this amount is sufficient to see the patterns we need to see.

### Text-Mining

#### Create Corpus

The first step is creating a corpus of text from the raw data:

```{r}
corpus <- VCorpus(DirSource("./data/samples"), readerControl = list(language = "en_US"))
```

#### Cleanup Corpus

We are required to remove some unwanted symbols from the text now (white space, punctuation, numbers, etc). We are also going to transform every words to lowercase and remove stop words:

```{r}
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, stopwords("english"))
```

#### Stemming

I'm not so sure at this moment that I should be doing stemming. I'm leaving the command here for now for future reference (evaluation for this chunk is turned off).

```{r, eval = FALSE}
corpus <- tm_map(corpus, stemDocument, language = "english")
```

#### Remove Curse Words

The most legitimate source for curse words I could find is [this project on GitHub](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words). This project maintains lists of curse words in several languages, including the ones we require for our data-sets.

```{r}
load_curse_words <- function(curse_words_url) {
    connection <- url(curse_words_url)
    lines <- readLines(connection)
    close(connection)
    stemDocument(lines)
}

curse_words <- load_curse_words(
    "https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en"
    )

corpus_without_curse_words <- tm_map(corpus, removeWords, curse_words)
```

## Exploratory Data Analysis

### Distribution of N-Grams of Size from One to Four Words

```{r}
create_words_data_frame <- function(document_term_matrix) {
  frequencies <- colSums(document_term_matrix)
  words <- data.frame(word = names(frequencies), frequency = frequencies)
  words <- arrange(words, desc(frequency))
  rownames(words) <- 1:length(frequencies)
  return(words)
}

one_to_four_ngrams <- function(x) 
  NGramTokenizer(x, Weka_control(min = 1, max = 4))

document_term_matrix <- DocumentTermMatrix(corpus_without_curse_words,
                                           control = list(tokenize = one_to_four_ngrams))

one_to_four_ngrams_words <- create_words_data_frame(as.matrix(document_term_matrix))
top_one_to_four_ngrams_words <- one_to_four_ngrams_words[1:50, ]
top_one_to_four_ngrams_words
```

Here's a bar chart for the top words in the distribution:

```{r, fig.height = 6, fig.width = 12}
word_frequency_barchart <-function(words)
    ggplot(words, aes(x = word, y = frequency)) +
        geom_bar(stat="identity", fill = "burlywood1") +
        scale_x_discrete(limits = words$word) +
        ggtitle(paste("Top", nrow(words), "Word Frequencies in the Corpus")) +
        ylab("Frequency") +
        theme(axis.text.x = element_text(size = 12, angle = 90, hjust = 1, vjust = 0.5)) +
        theme(plot.title = element_text(size = 18, face = "bold",
                                        hjust = 0.5, margin = margin(b = 30, unit = "pt"))) +
        theme(axis.title.x = element_blank()) +
        theme(axis.title.y = element_text(size = 14, face="bold")) +
        theme(panel.background = element_blank(), axis.line = element_line(colour = "black")) +
        theme(panel.border = element_rect(colour = "black", fill = NA, size = 0.5)) +
        theme(strip.background = element_rect(fill = alpha("burlywood3", 0.3), color = "black", size = 0.5)) +
        theme(legend.title = element_blank())

word_frequency_barchart(top_one_to_four_ngrams_words)
```

A word cloud is also a good visualization tool for n-gram frequency:

```{r, fig.height = 6, fig.width = 6}
words_cloud <- function(words)
    wordcloud(words$word, words$frequency, scale = c(4, 0.5), colors = brewer.pal(8, "Dark2"))

words_cloud(top_one_to_four_ngrams_words)
```

### Distribution of N-Grams of Size Two Words

```{r}
two_only_ngrams <- function(x) 
  NGramTokenizer(x, Weka_control(min = 2, max = 2))

document_term_matrix <- DocumentTermMatrix(corpus_without_curse_words,
                                           control = list(tokenize = two_only_ngrams))

two_only_ngrams_words <- create_words_data_frame(as.matrix(document_term_matrix))

top_two_only_ngrams_words <- two_only_ngrams_words[1:50, ]
top_two_only_ngrams_words
```

Its bar chart:

```{r, fig.height = 6, fig.width = 12}
word_frequency_barchart(top_two_only_ngrams_words)
```

Its word cloud:

```{r, fig.height = 9, fig.width = 9}
words_cloud(top_two_only_ngrams_words)
```

### Distribution of N-Grams of Size Three Words

```{r}
three_only_ngrams <- function(x) 
  NGramTokenizer(x, Weka_control(min = 3, max = 3))

document_term_matrix <- DocumentTermMatrix(corpus_without_curse_words,
                                           control = list(tokenize = three_only_ngrams))

three_only_ngrams_words <- create_words_data_frame(as.matrix(document_term_matrix))

top_three_only_ngrams_words <- three_only_ngrams_words[1:50, ]
top_three_only_ngrams_words
```

Its bar chart:

```{r, fig.height = 6, fig.width = 12}
word_frequency_barchart(top_three_only_ngrams_words)
```

Its word cloud:

```{r, fig.height = 9, fig.width = 9}
words_cloud(top_three_only_ngrams_words)
```

### Word Coverage

How Many of the top words are required to cover a given percentage of words present in the entire data-set?

We are going to answer this question by finding out how the number of words covered in the data-set increases as we add words from the most frequent to the least frequent:

```{r}
words_coverage <- data.frame(
    coverage = round(cumsum(one_to_four_ngrams_words$frequency) / sum(one_to_four_ngrams_words$frequency) * 100, 2),
    words = 1:nrow(one_to_four_ngrams_words)
    )

words_coverage[1:6, ]
```

Let's plot the coverage function as a line plot:

```{r, fig.height = 6, fig.width = 12}
ggplot(words_coverage, aes(x = words, y = coverage)) +
    geom_area(colour = "burlywood2", fill = "burlywood1", size = 1, alpha = 0.3) +
    ggtitle("Word Coverage in the Data-set vs. Top Occurring N-Grams Added") +
    xlab("Top Occuring N-Grams Added") +
    ylab("Percentage of Coverage") +
    theme(plot.title = element_text(size = 16, face = "bold",
                                    hjust = 0.5, margin = margin(b = 30, unit = "pt"))) +
    theme(axis.title.x = element_text(size = 12, face="bold")) +
    theme(axis.title.y = element_text(size = 12, face="bold")) +
    theme(panel.background = element_blank(), axis.line = element_line(colour = "black")) +
    theme(panel.border = element_rect(colour = "black", fill = NA, size = 0.5)) +
    theme(strip.background = element_rect(fill = alpha("burlywood3", 0.3), color = "black", size = 0.5))
```

This function that we have obtained experimentally from the data is named [Zipf CDF](https://en.wikipedia.org/wiki/Zipf%27s_law) (Cumulative Distribution Function). It is a well known fact that the distribution of words in natural languages follows the distribution described by this function.

Now we need to know what's the minimum number of top words added to achieve 50% and 90% coverage:

```{r}
number_ngrams_for_50_percent_coverage <- min(words_coverage[words_coverage$coverage > 50, ]$words)
number_ngrams_for_50_percent_coverage
```

```{r}
number_ngrams_for_90_percent_coverage <- min(words_coverage[words_coverage$coverage > 90, ]$words)
number_ngrams_for_90_percent_coverage
```

According with the computation, we would need `r number_ngrams_for_50_percent_coverage` words to achieve 50% coverage (that is, that's the minimum number of top words added that would represent half of the words present in the data) and `r number_ngrams_for_90_percent_coverage` words to achieve 90%.

### Language Detection

#### Language Detection Using TextCat

How about estimating the number of words in the data that are actually written in the English language:

```{r}
one_ngrams <- function(x) 
  NGramTokenizer(x, Weka_control(min = 1, max = 1))

document_term_matrix <- DocumentTermMatrix(corpus_without_curse_words,
                                           control = list(tokenize = one_ngrams))

one_ngrams_words <- create_words_data_frame(as.matrix(document_term_matrix))
one_ngrams_words_dt <- data.table(one_ngrams_words)
```

I'm using data tables due to performance. Much faster than using `sapply()` on data frames.

The R package [textcat](https://www.rdocumentation.org/packages/textcat/versions/1.0-4) uses machine learning to detect several languages:

```{r}
names(TC_byte_profiles)
```

This package is quite flexible and allows detection using several different profiles. Here's another one:

```{r}
names(ECIMCI_profiles)
```

You may also exclude languages from a particular profile:

```{r}
languages <- c("english", "german", "finnish", "russian-iso8859_5",   "russian-koi8_r", "russian-windows1251")
sub_tc_byte_profiles <- TC_byte_profiles[names(TC_byte_profiles) %in% languages]
names(sub_tc_byte_profiles)
```

You may then use these profiles for detection:

```{r}
language_dt <- one_ngrams_words_dt[, language := textcat(word, p = sub_tc_byte_profiles)]
language_aggregated_dt <- language_dt[, sum(frequency), by = language]
names(language_aggregated_dt) <- c("language", "frequency")
language_aggregated_dt <- language_aggregated_dt[, percentage := round(frequency / sum(language_dt[, 2]) * 100, 2)]
as.data.frame(language_aggregated_dt)
```

Looking at the classification for each word, it seems like textcat's accuracy leaves to desire:

```{r}
language_dt[1:10, ]
```

Thus, I decided to experiment with a different method, which follows in the next section.

#### Language Detection Using WordNet

The following function uses the wordnet package to determine if a word belongs to the English language:

```{r}
synonyms_found <- function(word, pos)
    length(synonyms(word, pos)) > 0

vectorized_synonyms <- function(words, pos)
    sapply(words, synonyms_found, pos = pos)

is_english <- function(input_word) {
    test_adjective <- vectorized_synonyms(word = singularize(input_word), "ADJECTIVE")
    test_adverb <- vectorized_synonyms(word = singularize(input_word), "ADVERB")
    test_noun <- vectorized_synonyms(word = singularize(input_word), "NOUN")
    test_verb <- vectorized_synonyms(word = singularize(input_word), "VERB")
    test_adjective | test_adverb | test_noun | test_verb
}

is_english_dt <- one_ngrams_words_dt[, is_english := is_english(word)]
is_english_aggregated_dt <- is_english_dt[, sum(frequency), by = is_english]
names(is_english_aggregated_dt) <- c("is_english", "frequency")
is_english_aggregated_dt <- is_english_aggregated_dt[, percentage := round(frequency / sum(language_dt[, 2]) * 100, 2)]
as.data.frame(is_english_aggregated_dt)
```

These numbers seem more reasonable, as well as the classification for each word:

```{r}
is_english_dt[1:10, ]
```

## Results

We have confirmed that the distribution of words follow some known theoretical properties by analyzing a randomly sampled sub-set of the available data. We are going to exploit such properties to build a prediction model for keyboard typing.

Regarding coverage, we would need `r number_ngrams_for_50_percent_coverage` words to achieve 50% coverage and `r number_ngrams_for_90_percent_coverage` words to achieve 90%. Coverage follows a Zipf CDF, a known probability distribution function.

We could increase coverage by choosing words more aligned with the vocabulary of the specific user for which we want to do keyboard typing prediction. We could do that by the following means:

- Improving the data corpus by using text collected from each individual user (essentially each user would start with a standard corpus of text, which would improve as the user types new sentences). Each user would have its own particular corpus of text (or its own model, which would be updated over time). That could raise privacy concerns though, as we would be required to store at least bits of conversations and web browsing behavior. That wouldn't be critical for public services such as blogs and twitter, but could be potentially a problem for news (we would need to keep track of articles the user has clicked on).

- Creating groups of users into different profiles (casual, business, scientific, etc) and respective models. An option would be to give the user the option of choosing an specific profile on the fly according with their current needs or commit to a single one when they sign-up for the service.

